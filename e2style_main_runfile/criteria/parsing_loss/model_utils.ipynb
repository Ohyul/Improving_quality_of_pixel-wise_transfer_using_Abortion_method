{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class conv2DBatchNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        n_filters,\n",
    "        k_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        bias=True,\n",
    "        dilation=1,\n",
    "        is_batchnorm=True,\n",
    "    ):\n",
    "        super(conv2DBatchNorm, self).__init__()\n",
    "\n",
    "        conv_mod = nn.Conv2d(int(in_channels),\n",
    "                             int(n_filters),\n",
    "                             kernel_size=k_size,\n",
    "                             padding=padding,\n",
    "                             stride=stride,\n",
    "                             bias=bias,\n",
    "                             dilation=dilation,)\n",
    "\n",
    "        if is_batchnorm:\n",
    "            self.cb_unit = nn.Sequential(conv_mod, nn.BatchNorm2d(int(n_filters)))\n",
    "        else:\n",
    "            self.cb_unit = nn.Sequential(conv_mod)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.cb_unit(inputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class conv2DGroupNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        n_filters,\n",
    "        k_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        bias=True,\n",
    "        dilation=1,\n",
    "        n_groups=16,\n",
    "    ):\n",
    "        super(conv2DGroupNorm, self).__init__()\n",
    "\n",
    "        conv_mod = nn.Conv2d(int(in_channels),\n",
    "                             int(n_filters),\n",
    "                             kernel_size=k_size,\n",
    "                             padding=padding,\n",
    "                             stride=stride,\n",
    "                             bias=bias,\n",
    "                             dilation=dilation,)\n",
    "\n",
    "        self.cg_unit = nn.Sequential(conv_mod, \n",
    "                                     nn.GroupNorm(n_groups, int(n_filters)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.cg_unit(inputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class deconv2DBatchNorm(nn.Module):\n",
    "    def __init__(self, in_channels, n_filters, k_size, stride, padding, bias=True):\n",
    "        super(deconv2DBatchNorm, self).__init__()\n",
    "\n",
    "        self.dcb_unit = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                int(in_channels),\n",
    "                int(n_filters),\n",
    "                kernel_size=k_size,\n",
    "                padding=padding,\n",
    "                stride=stride,\n",
    "                bias=bias,\n",
    "            ),\n",
    "            nn.BatchNorm2d(int(n_filters)),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.dcb_unit(inputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class conv2DBatchNormRelu(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        n_filters,\n",
    "        k_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        bias=True,\n",
    "        dilation=1,\n",
    "        is_batchnorm=True,\n",
    "    ):\n",
    "        super(conv2DBatchNormRelu, self).__init__()\n",
    "\n",
    "        conv_mod = nn.Conv2d(int(in_channels),\n",
    "                             int(n_filters),\n",
    "                             kernel_size=k_size,\n",
    "                             padding=padding,\n",
    "                             stride=stride,\n",
    "                             bias=bias,\n",
    "                             dilation=dilation,)\n",
    "\n",
    "        if is_batchnorm:\n",
    "            self.cbr_unit = nn.Sequential(conv_mod, \n",
    "                                          nn.BatchNorm2d(int(n_filters)), \n",
    "                                          nn.ReLU(inplace=True))\n",
    "        else:\n",
    "            self.cbr_unit = nn.Sequential(conv_mod, nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.cbr_unit(inputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class conv2DGroupNormRelu(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        n_filters,\n",
    "        k_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        bias=True,\n",
    "        dilation=1,\n",
    "        n_groups=16,\n",
    "    ):\n",
    "        super(conv2DGroupNormRelu, self).__init__()\n",
    "\n",
    "        conv_mod = nn.Conv2d(int(in_channels),\n",
    "                             int(n_filters),\n",
    "                             kernel_size=k_size,\n",
    "                             padding=padding,\n",
    "                             stride=stride,\n",
    "                             bias=bias,\n",
    "                             dilation=dilation,)\n",
    "\n",
    "        self.cgr_unit = nn.Sequential(conv_mod, \n",
    "                                      nn.GroupNorm(n_groups, int(n_filters)), \n",
    "                                      nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.cgr_unit(inputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "class deconv2DBatchNormRelu(nn.Module):\n",
    "    def __init__(self, in_channels, n_filters, k_size, stride, padding, bias=True):\n",
    "        super(deconv2DBatchNormRelu, self).__init__()\n",
    "\n",
    "        self.dcbr_unit = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                int(in_channels),\n",
    "                int(n_filters),\n",
    "                kernel_size=k_size,\n",
    "                padding=padding,\n",
    "                stride=stride,\n",
    "                bias=bias,\n",
    "            ),\n",
    "            nn.BatchNorm2d(int(n_filters)),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.dcbr_unit(inputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class unetConv2(nn.Module):\n",
    "    def __init__(self, in_size, out_size, is_batchnorm):\n",
    "        super(unetConv2, self).__init__()\n",
    "\n",
    "        if is_batchnorm:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(in_size, out_size, 3, 1, 1),\n",
    "                nn.BatchNorm2d(out_size),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.Conv2d(out_size, out_size, 3, 1, 1),\n",
    "                nn.BatchNorm2d(out_size),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(nn.Conv2d(in_size, out_size, 3, 1, 1), nn.ReLU())\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.Conv2d(out_size, out_size, 3, 1, 1), nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.conv1(inputs)\n",
    "        #print (outputs.shape)\n",
    "        outputs = self.conv2(outputs)\n",
    "        #print (outputs.shape)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class unetUp(nn.Module):\n",
    "    def __init__(self, in_size, out_size, is_deconv, is_batchnorm):\n",
    "        super(unetUp, self).__init__()\n",
    "        self.conv = unetConv2(in_size, out_size, is_batchnorm)\n",
    "        if is_deconv:\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
    "        else:\n",
    "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "\n",
    "    def forward(self, inputs1, inputs2):\n",
    "        outputs2 = self.up(inputs2)\n",
    "        offset = outputs2.size()[2] - inputs1.size()[2]\n",
    "        padding = 2 * [offset // 2, offset // 2]           \n",
    "        outputs1 = F.pad(inputs1, padding)\n",
    "               \n",
    "        return self.conv(torch.cat([outputs1, outputs2], 1))\n",
    "\n",
    "\n",
    "class segnetDown2(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(segnetDown2, self).__init__()\n",
    "        self.conv1 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
    "        self.conv2 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n",
    "        self.maxpool_with_argmax = nn.MaxPool2d(2, 2, return_indices=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.conv1(inputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        unpooled_shape = outputs.size()\n",
    "        outputs, indices = self.maxpool_with_argmax(outputs)\n",
    "        return outputs, indices, unpooled_shape\n",
    "\n",
    "\n",
    "class segnetDown3(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(segnetDown3, self).__init__()\n",
    "        self.conv1 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
    "        self.conv2 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n",
    "        self.conv3 = conv2DBatchNormRelu(out_size, out_size, 3, 1, 1)\n",
    "        self.maxpool_with_argmax = nn.MaxPool2d(2, 2, return_indices=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.conv1(inputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        outputs = self.conv3(outputs)\n",
    "        unpooled_shape = outputs.size()\n",
    "        outputs, indices = self.maxpool_with_argmax(outputs)\n",
    "        return outputs, indices, unpooled_shape\n",
    "\n",
    "\n",
    "class segnetUp2(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(segnetUp2, self).__init__()\n",
    "        self.unpool = nn.MaxUnpool2d(2, 2)\n",
    "        self.conv1 = conv2DBatchNormRelu(in_size, in_size, 3, 1, 1)\n",
    "        self.conv2 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
    "\n",
    "    def forward(self, inputs, indices, output_shape):\n",
    "        outputs = self.unpool(input=inputs, indices=indices, output_size=output_shape)\n",
    "        outputs = self.conv1(outputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class segnetUp3(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(segnetUp3, self).__init__()\n",
    "        self.unpool = nn.MaxUnpool2d(2, 2)\n",
    "        self.conv1 = conv2DBatchNormRelu(in_size, in_size, 3, 1, 1)\n",
    "        self.conv2 = conv2DBatchNormRelu(in_size, in_size, 3, 1, 1)\n",
    "        self.conv3 = conv2DBatchNormRelu(in_size, out_size, 3, 1, 1)\n",
    "\n",
    "    def forward(self, inputs, indices, output_shape):\n",
    "        outputs = self.unpool(input=inputs, indices=indices, output_size=output_shape)\n",
    "        outputs = self.conv1(outputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        outputs = self.conv3(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class residualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, n_filters, stride=1, downsample=None):\n",
    "        super(residualBlock, self).__init__()\n",
    "\n",
    "        self.convbnrelu1 = conv2DBatchNormRelu(\n",
    "            in_channels, n_filters, 3, stride, 1, bias=False\n",
    "        )\n",
    "        self.convbn2 = conv2DBatchNorm(n_filters, n_filters, 3, 1, 1, bias=False)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.convbnrelu1(x)\n",
    "        out = self.convbn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class residualBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, n_filters, stride=1, downsample=None):\n",
    "        super(residualBottleneck, self).__init__()\n",
    "        self.convbn1 = nn.Conv2DBatchNorm(in_channels, n_filters, k_size=1, bias=False)\n",
    "        self.convbn2 = nn.Conv2DBatchNorm(\n",
    "            n_filters, n_filters, k_size=3, padding=1, stride=stride, bias=False\n",
    "        )\n",
    "        self.convbn3 = nn.Conv2DBatchNorm(\n",
    "            n_filters, n_filters * 4, k_size=1, bias=False\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.convbn1(x)\n",
    "        out = self.convbn2(out)\n",
    "        out = self.convbn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class linknetUp(nn.Module):\n",
    "    def __init__(self, in_channels, n_filters):\n",
    "        super(linknetUp, self).__init__()\n",
    "\n",
    "        # B, 2C, H, W -> B, C/2, H, W\n",
    "        self.convbnrelu1 = conv2DBatchNormRelu(\n",
    "            in_channels, n_filters / 2, k_size=1, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        # B, C/2, H, W -> B, C/2, H, W\n",
    "        self.deconvbnrelu2 = nn.deconv2DBatchNormRelu(\n",
    "            n_filters / 2, n_filters / 2, k_size=3, stride=2, padding=0\n",
    "        )\n",
    "\n",
    "        # B, C/2, H, W -> B, C, H, W\n",
    "        self.convbnrelu3 = conv2DBatchNormRelu(\n",
    "            n_filters / 2, n_filters, k_size=1, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convbnrelu1(x)\n",
    "        x = self.deconvbnrelu2(x)\n",
    "        x = self.convbnrelu3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FRRU(nn.Module):\n",
    "    \"\"\"\n",
    "    Full Resolution Residual Unit for FRRN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 prev_channels, \n",
    "                 out_channels, \n",
    "                 scale, \n",
    "                 group_norm=False,\n",
    "                 n_groups=None):\n",
    "        super(FRRU, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.prev_channels = prev_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.group_norm = group_norm\n",
    "        self.n_groups = n_groups\n",
    "\n",
    "\n",
    "        if self.group_norm:\n",
    "            conv_unit = conv2DGroupNormRelu\n",
    "            self.conv1 = conv_unit(\n",
    "                prev_channels + 32, out_channels, k_size=3, \n",
    "                stride=1, padding=1, bias=False, n_groups=self.n_groups\n",
    "            )\n",
    "            self.conv2 = conv_unit(\n",
    "                out_channels, out_channels, k_size=3, \n",
    "                stride=1, padding=1, bias=False, n_groups=self.n_groups\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            conv_unit = conv2DBatchNormRelu\n",
    "            self.conv1 = conv_unit(prev_channels + 32, out_channels, k_size=3, \n",
    "                                   stride=1, padding=1, bias=False,)\n",
    "            self.conv2 = conv_unit(out_channels, out_channels, k_size=3, \n",
    "                                   stride=1, padding=1, bias=False,)\n",
    "\n",
    "        self.conv_res = nn.Conv2d(out_channels, 32, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, y, z):\n",
    "        x = torch.cat([y, nn.MaxPool2d(self.scale, self.scale)(z)], dim=1)\n",
    "        y_prime = self.conv1(x)\n",
    "        y_prime = self.conv2(y_prime)\n",
    "\n",
    "        x = self.conv_res(y_prime)\n",
    "        upsample_size = torch.Size([_s * self.scale for _s in y_prime.shape[-2:]])\n",
    "        x = F.upsample(x, size=upsample_size, mode=\"nearest\")\n",
    "        z_prime = z + x\n",
    "\n",
    "        return y_prime, z_prime\n",
    "\n",
    "\n",
    "class RU(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Unit for FRRN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 channels, \n",
    "                 kernel_size=3, \n",
    "                 strides=1, \n",
    "                 group_norm=False,\n",
    "                 n_groups=None):\n",
    "        super(RU, self).__init__()\n",
    "        self.group_norm = group_norm\n",
    "        self.n_groups = n_groups\n",
    "\n",
    "        if self.group_norm:\n",
    "            self.conv1 = conv2DGroupNormRelu(\n",
    "               channels, channels, k_size=kernel_size, \n",
    "               stride=strides, padding=1, bias=False,n_groups=self.n_groups)\n",
    "            self.conv2 = conv2DGroupNorm(\n",
    "                channels, channels, k_size=kernel_size, \n",
    "                stride=strides, padding=1, bias=False,n_groups=self.n_groups)\n",
    "\n",
    "        else:\n",
    "            self.conv1 = conv2DBatchNormRelu(\n",
    "               channels, channels, k_size=kernel_size, stride=strides, padding=1, bias=False,)\n",
    "            self.conv2 = conv2DBatchNorm(\n",
    "                channels, channels, k_size=kernel_size, stride=strides, padding=1, bias=False,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        incoming = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x + incoming\n",
    "\n",
    "\n",
    "class residualConvUnit(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super(residualConvUnit, self).__init__()\n",
    "\n",
    "        self.residual_conv_unit = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=kernel_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=kernel_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.residual_conv_unit(x)\n",
    "        return x + input\n",
    "\n",
    "\n",
    "class multiResolutionFusion(nn.Module):\n",
    "    def __init__(self, channels, up_scale_high, up_scale_low, high_shape, low_shape):\n",
    "        super(multiResolutionFusion, self).__init__()\n",
    "\n",
    "        self.up_scale_high = up_scale_high\n",
    "        self.up_scale_low = up_scale_low\n",
    "\n",
    "        self.conv_high = nn.Conv2d(high_shape[1], channels, kernel_size=3)\n",
    "\n",
    "        if low_shape is not None:\n",
    "            self.conv_low = nn.Conv2d(low_shape[1], channels, kernel_size=3)\n",
    "\n",
    "    def forward(self, x_high, x_low):\n",
    "        high_upsampled = F.upsample(\n",
    "            self.conv_high(x_high), scale_factor=self.up_scale_high, mode=\"bilinear\"\n",
    "        )\n",
    "\n",
    "        if x_low is None:\n",
    "            return high_upsampled\n",
    "\n",
    "        low_upsampled = F.upsample(\n",
    "            self.conv_low(x_low), scale_factor=self.up_scale_low, mode=\"bilinear\"\n",
    "        )\n",
    "\n",
    "        return low_upsampled + high_upsampled\n",
    "\n",
    "\n",
    "class chainedResidualPooling(nn.Module):\n",
    "    def __init__(self, channels, input_shape):\n",
    "        super(chainedResidualPooling, self).__init__()\n",
    "\n",
    "        self.chained_residual_pooling = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(5, 1, 2),\n",
    "            nn.Conv2d(input_shape[1], channels, kernel_size=3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.chained_residual_pooling(x)\n",
    "        return x + input\n",
    "\n",
    "\n",
    "class pyramidPooling(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        pool_sizes,\n",
    "        model_name=\"pspnet\",\n",
    "        fusion_mode=\"cat\",\n",
    "        is_batchnorm=True,\n",
    "    ):\n",
    "        super(pyramidPooling, self).__init__()\n",
    "\n",
    "        bias = not is_batchnorm\n",
    "\n",
    "        self.paths = []\n",
    "        for i in range(len(pool_sizes)):\n",
    "            self.paths.append(\n",
    "                conv2DBatchNormRelu(\n",
    "                    in_channels,\n",
    "                    int(in_channels / len(pool_sizes)),\n",
    "                    1,\n",
    "                    1,\n",
    "                    0,\n",
    "                    bias=bias,\n",
    "                    is_batchnorm=is_batchnorm,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.path_module_list = nn.ModuleList(self.paths)\n",
    "        self.pool_sizes = pool_sizes\n",
    "        self.model_name = model_name\n",
    "        self.fusion_mode = fusion_mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[2:]\n",
    "\n",
    "        if self.training or self.model_name != \"icnet\":  # general settings or pspnet\n",
    "            k_sizes = []\n",
    "            strides = []\n",
    "            for pool_size in self.pool_sizes:\n",
    "                k_sizes.append((int(h / pool_size), int(w / pool_size)))\n",
    "                strides.append((int(h / pool_size), int(w / pool_size)))\n",
    "        else:  # eval mode and icnet: pre-trained for 1025 x 2049\n",
    "            k_sizes = [(8, 15), (13, 25), (17, 33), (33, 65)]\n",
    "            strides = [(5, 10), (10, 20), (16, 32), (33, 65)]\n",
    "\n",
    "        if self.fusion_mode == \"cat\":  # pspnet: concat (including x)\n",
    "            output_slices = [x]\n",
    "\n",
    "            for i, (module, pool_size) in enumerate(\n",
    "                zip(self.path_module_list, self.pool_sizes)\n",
    "            ):\n",
    "                out = F.avg_pool2d(x, k_sizes[i], stride=strides[i], padding=0)\n",
    "                # out = F.adaptive_avg_pool2d(x, output_size=(pool_size, pool_size))\n",
    "                if self.model_name != \"icnet\":\n",
    "                    out = module(out)\n",
    "                out = F.interpolate(out, size=(h, w), mode=\"bilinear\", align_corners=True)\n",
    "                output_slices.append(out)\n",
    "\n",
    "            return torch.cat(output_slices, dim=1)\n",
    "        else:  # icnet: element-wise sum (including x)\n",
    "            pp_sum = x\n",
    "\n",
    "            for i, (module, pool_size) in enumerate(\n",
    "                zip(self.path_module_list, self.pool_sizes)\n",
    "            ):\n",
    "                out = F.avg_pool2d(x, k_sizes[i], stride=strides[i], padding=0)\n",
    "                # out = F.adaptive_avg_pool2d(x, output_size=(pool_size, pool_size))\n",
    "                if self.model_name != \"icnet\":\n",
    "                    out = module(out)\n",
    "                out = F.interpolate(out, size=(h, w), mode=\"bilinear\", align_corners=True)\n",
    "                pp_sum = pp_sum + out\n",
    "\n",
    "            return pp_sum\n",
    "\n",
    "\n",
    "class bottleNeckPSP(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, mid_channels, out_channels, stride, dilation=1, is_batchnorm=True\n",
    "    ):\n",
    "        super(bottleNeckPSP, self).__init__()\n",
    "\n",
    "        bias = not is_batchnorm\n",
    "\n",
    "        self.cbr1 = conv2DBatchNormRelu(\n",
    "            in_channels,\n",
    "            mid_channels,\n",
    "            1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=bias,\n",
    "            is_batchnorm=is_batchnorm,\n",
    "        )\n",
    "        if dilation > 1:\n",
    "            self.cbr2 = conv2DBatchNormRelu(\n",
    "                mid_channels,\n",
    "                mid_channels,\n",
    "                3,\n",
    "                stride=stride,\n",
    "                padding=dilation,\n",
    "                bias=bias,\n",
    "                dilation=dilation,\n",
    "                is_batchnorm=is_batchnorm,\n",
    "            )\n",
    "        else:\n",
    "            self.cbr2 = conv2DBatchNormRelu(\n",
    "                mid_channels,\n",
    "                mid_channels,\n",
    "                3,\n",
    "                stride=stride,\n",
    "                padding=1,\n",
    "                bias=bias,\n",
    "                dilation=1,\n",
    "                is_batchnorm=is_batchnorm,\n",
    "            )\n",
    "        self.cb3 = conv2DBatchNorm(\n",
    "            mid_channels,\n",
    "            out_channels,\n",
    "            1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=bias,\n",
    "            is_batchnorm=is_batchnorm,\n",
    "        )\n",
    "        self.cb4 = conv2DBatchNorm(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            1,\n",
    "            stride=stride,\n",
    "            padding=0,\n",
    "            bias=bias,\n",
    "            is_batchnorm=is_batchnorm,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.cb3(self.cbr2(self.cbr1(x)))\n",
    "        residual = self.cb4(x)\n",
    "        return F.relu(conv + residual, inplace=True)\n",
    "\n",
    "\n",
    "class bottleNeckIdentifyPSP(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, stride, dilation=1, is_batchnorm=True):\n",
    "        super(bottleNeckIdentifyPSP, self).__init__()\n",
    "\n",
    "        bias = not is_batchnorm\n",
    "\n",
    "        self.cbr1 = conv2DBatchNormRelu(\n",
    "            in_channels,\n",
    "            mid_channels,\n",
    "            1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=bias,\n",
    "            is_batchnorm=is_batchnorm,\n",
    "        )\n",
    "        if dilation > 1:\n",
    "            self.cbr2 = conv2DBatchNormRelu(\n",
    "                mid_channels,\n",
    "                mid_channels,\n",
    "                3,\n",
    "                stride=1,\n",
    "                padding=dilation,\n",
    "                bias=bias,\n",
    "                dilation=dilation,\n",
    "                is_batchnorm=is_batchnorm,\n",
    "            )\n",
    "        else:\n",
    "            self.cbr2 = conv2DBatchNormRelu(\n",
    "                mid_channels,\n",
    "                mid_channels,\n",
    "                3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=bias,\n",
    "                dilation=1,\n",
    "                is_batchnorm=is_batchnorm,\n",
    "            )\n",
    "        self.cb3 = conv2DBatchNorm(\n",
    "            mid_channels,\n",
    "            in_channels,\n",
    "            1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=bias,\n",
    "            is_batchnorm=is_batchnorm,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.cb3(self.cbr2(self.cbr1(x)))\n",
    "        return F.relu(x + residual, inplace=True)\n",
    "\n",
    "\n",
    "class residualBlockPSP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_blocks,\n",
    "        in_channels,\n",
    "        mid_channels,\n",
    "        out_channels,\n",
    "        stride,\n",
    "        dilation=1,\n",
    "        include_range=\"all\",\n",
    "        is_batchnorm=True,\n",
    "    ):\n",
    "        super(residualBlockPSP, self).__init__()\n",
    "\n",
    "        if dilation > 1:\n",
    "            stride = 1\n",
    "\n",
    "        # residualBlockPSP = convBlockPSP + identityBlockPSPs\n",
    "        layers = []\n",
    "        if include_range in [\"all\", \"conv\"]:\n",
    "            layers.append(\n",
    "                bottleNeckPSP(\n",
    "                    in_channels,\n",
    "                    mid_channels,\n",
    "                    out_channels,\n",
    "                    stride,\n",
    "                    dilation,\n",
    "                    is_batchnorm=is_batchnorm,\n",
    "                )\n",
    "            )\n",
    "        if include_range in [\"all\", \"identity\"]:\n",
    "            for i in range(n_blocks - 1):\n",
    "                layers.append(\n",
    "                    bottleNeckIdentifyPSP(\n",
    "                        out_channels, mid_channels, stride, dilation, is_batchnorm=is_batchnorm\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class cascadeFeatureFusion(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_classes, low_in_channels, high_in_channels, out_channels, is_batchnorm=True\n",
    "    ):\n",
    "        super(cascadeFeatureFusion, self).__init__()\n",
    "\n",
    "        bias = not is_batchnorm\n",
    "\n",
    "        self.low_dilated_conv_bn = conv2DBatchNorm(\n",
    "            low_in_channels,\n",
    "            out_channels,\n",
    "            3,\n",
    "            stride=1,\n",
    "            padding=2,\n",
    "            bias=bias,\n",
    "            dilation=2,\n",
    "            is_batchnorm=is_batchnorm,\n",
    "        )\n",
    "        self.low_classifier_conv = nn.Conv2d(\n",
    "            int(low_in_channels),\n",
    "            int(n_classes),\n",
    "            kernel_size=1,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            bias=True,\n",
    "            dilation=1,\n",
    "        )  # Train only\n",
    "        self.high_proj_conv_bn = conv2DBatchNorm(\n",
    "            high_in_channels,\n",
    "            out_channels,\n",
    "            1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=bias,\n",
    "            is_batchnorm=is_batchnorm,\n",
    "        )\n",
    "\n",
    "    def forward(self, x_low, x_high):\n",
    "        x_low_upsampled = F.interpolate(\n",
    "            x_low, size=get_interp_size(x_low, z_factor=2), mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "\n",
    "        low_cls = self.low_classifier_conv(x_low_upsampled)\n",
    "\n",
    "        low_fm = self.low_dilated_conv_bn(x_low_upsampled)\n",
    "        high_fm = self.high_proj_conv_bn(x_high)\n",
    "        high_fused_fm = F.relu(low_fm + high_fm, inplace=True)\n",
    "\n",
    "        return high_fused_fm, low_cls\n",
    "\n",
    "\n",
    "def get_interp_size(input, s_factor=1, z_factor=1):  # for caffe\n",
    "    ori_h, ori_w = input.shape[2:]\n",
    "\n",
    "    # shrink (s_factor >= 1)\n",
    "    ori_h = (ori_h - 1) / s_factor + 1\n",
    "    ori_w = (ori_w - 1) / s_factor + 1\n",
    "\n",
    "    # zoom (z_factor >= 1)\n",
    "    ori_h = ori_h + (ori_h - 1) * (z_factor - 1)\n",
    "    ori_w = ori_w + (ori_w - 1) * (z_factor - 1)\n",
    "\n",
    "    resize_shape = (int(ori_h), int(ori_w))\n",
    "    return resize_shape\n",
    "\n",
    "\n",
    "def interp(input, output_size, mode=\"bilinear\"):\n",
    "    n, c, ih, iw = input.shape\n",
    "    oh, ow = output_size\n",
    "\n",
    "    # normalize to [-1, 1]\n",
    "    h = torch.arange(0, oh, dtype=torch.float, device='cuda' if input.is_cuda else 'cpu') / (oh - 1) * 2 - 1\n",
    "    w = torch.arange(0, ow, dtype=torch.float, device='cuda' if input.is_cuda else 'cpu') / (ow - 1) * 2 - 1\n",
    "\n",
    "    grid = torch.zeros(oh, ow, 2, dtype=torch.float, device='cuda' if input.is_cuda else 'cpu')\n",
    "    grid[:, :, 0] = w.unsqueeze(0).repeat(oh, 1)\n",
    "    grid[:, :, 1] = h.unsqueeze(0).repeat(ow, 1).transpose(0, 1)\n",
    "    grid = grid.unsqueeze(0).repeat(n, 1, 1, 1)  # grid.shape: [n, oh, ow, 2]\n",
    "\n",
    "    return F.grid_sample(input, grid, mode=mode)\n",
    "\n",
    "\n",
    "def get_upsampling_weight(in_channels, out_channels, kernel_size):\n",
    "    \"\"\"Make a 2D bilinear kernel suitable for upsampling\"\"\"\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:kernel_size, :kernel_size]\n",
    "    filt = (1 - abs(og[0] - center) / factor) * \\\n",
    "           (1 - abs(og[1] - center) / factor)\n",
    "    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),\n",
    "                      dtype=np.float64)\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return torch.from_numpy(weight).float()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
